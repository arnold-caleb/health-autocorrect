# error_classification.yaml

# Main configurations
main:
  seed: 42
  checkpoint_folder: "/proj/vondrick/aa4870/error_identification_checkpoints_200"
  best_model_name: "/home/aa4870/spring/physionet.org/files/mimic-cxr-jpg/2.0.0/weights/error_identification_checkpoints_200.pth"
  batch_size: 64
  learning_rate: 3e-4
  epochs: 30
  patience: 500000
  checkpoint_interval: 5
  embed_dim: 2560
  alsentzer: "emilyalsentzer/Bio_ClinicalBERT" 
  gatortron_medium: "UFNLP/gatortron-medium" # https://www.nature.com/articles/s41746-022-00742-2/tables/2
  gatortron_large: "UFNLP/gatortron-large" # evaluation scores higher than bioBERT, clinicalBERT, bioMegatron # embed dim of gatortron is 2560...
  biomegatron: "EMBO/BioMegatron345mUncased"
  embed_dim_gatortron: 2560
  embed_dim_biomegatron: 1024
  device_ids: [0, 1, 2, 3]  

# Data transformations
data_transform:
  _target_: "dataset.DataTransformImageTextDataset"
  resize: 224
  center_crop: 224
  random_horizontal_flip_p: 0.5
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# DataLoader configurations
dataloader:
  shuffle: true
  num_workers: 8

# Dataset configurations
dataset:
  _target_: "dataset.TextImageDataset"

# Optimizer and Scheduler configurations
optimizer:
  _target_: "torch.optim.AdamW"
  lr: 3e-4
scheduler:
  _target_: "torch.optim.lr_scheduler.CosineAnnealingLR"
  T_max: 150

# WandB configurations
wandb:
  project: "ERROR CORRECTION TRAINING"
  name: "Error correction (again frustrating)"

# Model configurations
model:
  _target_: "models.ImageTextCorrection"
  embed_dim: 1024
  llm: "EMBO/BioMegatron345mUncased"
